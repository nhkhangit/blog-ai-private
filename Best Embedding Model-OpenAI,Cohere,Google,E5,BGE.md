# Best Embedding Model ğŸŒŸ â€” OpenAI / Cohere / Google / E5 / BGE

So sÃ¡nh chuyÃªn sÃ¢u vá» cÃ¡c mÃ´ hÃ¬nh nhÃºng Ä‘a ngÃ´n ngá»¯

![alt text](images-2/image.png)

### ChÃºng ta Ä‘ang á»Ÿ giá»¯a má»™t bÆ°á»›c ngoáº·t trong lÄ©nh vá»±c AI, vá»›i cÃ¡c cÃ´ng ty ngÃ y cÃ ng chuyá»ƒn tá»« viá»‡c chá»‰ dá»±a vÃ o cÃ¡c mÃ´ hÃ¬nh ná»™i bá»™ sang sá»­ dá»¥ng APIs tá»« cÃ¡c nhÃ  cung cáº¥p hÃ ng Ä‘áº§u toÃ n cáº§u.

CÃ¡c cÃ´ng ty nhÆ° OpenAI, Google, Cohere, vÃ  Anthropic hiá»‡n Ä‘ang thá»‘ng trá»‹ thá»‹ trÆ°á»ng LLM (Large Language Model) toÃ n cáº§u má»›i nÃ y, nháº±m giáº£i quyáº¿t cÃ¡c nhiá»‡m vá»¥ Natural Language Processing (NLP) trÃªn toÃ n tháº¿ giá»›i.

Song song vá»›i Ä‘Ã³, cÃ¡c Text Embedding API Ä‘á»™t phÃ¡, ráº¥t quan trá»ng cho nhiá»u á»©ng dá»¥ng khÃ¡c nhau, Ä‘Ã£ xuáº¥t hiá»‡n; vÃ  chÃºng ta Ä‘ang chá»©ng kiáº¿n má»™t cuá»™c chiáº¿n giá»¯a cÃ¡c gÃ£ khá»•ng lá»“ Ä‘á»ƒ cung cáº¥p dá»‹ch vá»¥ embedding Ä‘a ngÃ´n ngá»¯ tá»‘t nháº¥t.

Microsoft Ä‘Ã£ Ã¡p dá»¥ng má»™t cÃ¡ch tiáº¿p cáº­n Ä‘á»™c Ä‘Ã¡o báº±ng cÃ¡ch open-source má»™t mÃ´ hÃ¬nh embedding Ä‘a ngÃ´n ngá»¯ gá»i lÃ  E5, qua Ä‘Ã³ lÃ m tÄƒng Ä‘á»™ sÃ¢u cho mÃ´i trÆ°á»ng cáº¡nh tranh. VÃ  Viá»‡n TrÃ­ tuá»‡ NhÃ¢n táº¡o Báº¯c Kinh gáº§n Ä‘Ã¢y Ä‘Ã£ ra máº¯t má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ cáº¡nh tranh má»›i, Ä‘Æ°á»£c biáº¿t Ä‘áº¿n vá»›i tÃªn gá»i BGE-M3.

LÃ  má»™t Ká»¹ sÆ° Machine Learning giÃ u kinh nghiá»‡m chuyÃªn vá» phÃ¡t triá»ƒn sáº£n pháº©m cho sá»­ dá»¥ng Ä‘a ngÃ´n ngá»¯, tÃ´i tháº¥y nhá»¯ng tiáº¿n bá»™ nÃ y Ä‘áº·c biá»‡t thÃº vá»‹ vÃ  Ä‘Ã£ quyáº¿t Ä‘á»‹nh so sÃ¡nh cÃ¡c cÃ´ng nghá»‡ tiÃªn tiáº¿n nháº¥t trong lÄ©nh vá»±c nÃ y.

HÃ´m nay, tÃ´i sáº½ trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch hiá»‡u suáº¥t Ä‘á»™c láº­p cá»§a cÃ¡c mÃ´ hÃ¬nh embedding Ä‘a dáº¡ng táº­p trung vÃ o hiá»‡u quáº£ cá»§a chÃºng trÃªn cÃ¡c truy váº¥n trong nhiá»u ngÃ´n ngá»¯.

So sÃ¡nh nÃ y bao gá»“m cÃ¡c ná»n táº£ng hÃ ng Ä‘áº§u OpenAI, Google, vÃ  Cohere, cÃ¹ng vá»›i cÃ¡c mÃ´ hÃ¬nh open-source cÃ³ hiá»‡u suáº¥t hÃ ng Ä‘áº§u, Ä‘á»ƒ lÃ m ná»•i báº­t sá»©c máº¡nh tÆ°Æ¡ng Ä‘á»‘i cá»§a chÃºng trong bá»‘i cáº£nh AI Ä‘ang phÃ¡t triá»ƒn nhanh chÃ³ng!

Note: Náº¿u nhá»¯ng bÃ i viáº¿t nhÆ° tháº¿ nÃ y khÆ¡i gá»£i sá»± quan tÃ¢m cá»§a báº¡n, hÃ£y cÃ¢n nháº¯c theo dÃµi tÃ´i Ä‘á»ƒ nháº­n Ä‘Æ°á»£c nhiá»u thÃ´ng tin chi tiáº¿t hÆ¡n trong tÆ°Æ¡ng lai!

![alt text](images-2/image-1.png)


### Models To Compare âš–ï¸

TÃ´i Ä‘Ã£ bao gá»“m cÃ¡c mÃ´ hÃ¬nh ná»•i báº­t nháº¥t cho sá»± so sÃ¡nh nÃ y â€” bao gá»“m cáº£ closed-sourced vÃ  open-source. DÆ°á»›i Ä‘Ã¢y lÃ  tá»•ng quan vá» cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau cÃ¹ng vá»›i vÃ­ dá»¥ vá» cÃ¡ch tÃ´i sá»­ dá»¥ng chÃºng.

#### OpenAI Embeddings
OpenAI cung cáº¥p má»™t closed-sourced API cho cÃ¡c text embeddings Ä‘a ngÃ´n ngá»¯.

MÃ´ hÃ¬nh embedding má»›i nháº¥t cá»§a há», `text-embedding-3-large`, Ä‘Æ°á»£c phÃ¡t hÃ nh vÃ o ngÃ y 25 thÃ¡ng 1, 2024, lÃ  má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ gá»‘c vÃ  há»— trá»£ 256, 1024, vÃ  3072 dimensions.

Máº·c Ä‘á»‹nh, `text-embedding-3-large` tráº£ vá» embedding vá»›i 3072 dimensions.

##### VÃ­ dá»¥ sá»­ dá»¥ng:

```python
from openai import OpenAI
import os

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

def openai_embed(query: str, model="text-embedding-3-large"):
    query = query.replace("\n", " ")
    response = openai_client.embeddings.create(input=[query], model=model)
    embedding = response.data[0].embedding
    return embedding

# Example usage (3072 dimensions)
embeddings = openai_embed("This is a text I want to embed")
```

Xem tÃ i liá»‡u: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)

API documentation cho dimension: [https://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-dimensions](https://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-dimensions)

### Cohere Embeddings

Cohere cung cáº¥p má»™t closed-sourced API cho cÃ¡c text embeddings Ä‘a ngÃ´n ngá»¯.

MÃ´ hÃ¬nh embedding má»›i nháº¥t cá»§a há», `embed-multilingual-v3.0`, Ä‘Æ°á»£c phÃ¡t hÃ nh vÃ o ngÃ y 2 thÃ¡ng 11, 2023, lÃ  má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ vÃ  tráº£ vá» embedding vá»›i 1024 dimensions.

#### VÃ­ dá»¥ sá»­ dá»¥ng:

```python
import cohere
import os

COHERE_API_KEY = os.environ.get("COHERE_API_KEY", "")
cohere_client = cohere.Client(COHERE_API_KEY)

def cohere_embed(query: str, model="embed-multilingual-v3.0"):
    response = cohere_client.embed(
      texts=[query],
      input_type="search_query",
      model=model
    )
    embedding = response.embeddings[0]
    return embedding

# Example usage (1024 dimensions)
embeddings = cohere_embed('This is a text I want to embed')
```

Xem tÃ i liá»‡u: [https://docs.cohere.com/docs/multilingual-language-models](https://docs.cohere.com/docs/multilingual-language-models)

### Google Embeddings

Google cung cáº¥p má»™t closed-sourced API cho cÃ¡c text embeddings Ä‘a ngÃ´n ngá»¯.

MÃ´ hÃ¬nh embedding má»›i nháº¥t cá»§a há», `text-multilingual-embedding-preview-0409`, Ä‘Æ°á»£c phÃ¡t hÃ nh dÆ°á»›i dáº¡ng preview vÃ o ngÃ y 2 thÃ¡ng 4, 2024, lÃ  má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ vÃ  tráº£ vá» embedding vá»›i 768 dimensions.

PhiÃªn báº£n nÃ y lÃ  má»™t cáº£i tiáº¿n so vá»›i mÃ´ hÃ¬nh embedding trÆ°á»›c Ä‘Ã³ tá»« series gecko `textembedding-gecko-multilingual@001`.

#### VÃ­ dá»¥ sá»­ dá»¥ng:

```python
from vertexai.preview.language_models import TextEmbeddingModel

def google_embed(query: str):
    embedder_name = "text-multilingual-embedding-preview-0409"
    model = TextEmbeddingModel.from_pretrained(embedder_name)
    embeddings_list = model.get_embeddings([query])
    embeddings = embeddings_list[0].values
    return embeddings

# Example usage (768 dimensions)
embeddings = google_embed("This is a text I want to embed")
```

LiÃªn káº¿t tá»›i tÃ i liá»‡u: [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#latest_models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#latest_models)

### Microsoftâ€™s E5 Embeddings

Multilingual E5 lÃ  má»™t mÃ´ hÃ¬nh embedding Ä‘a ngÃ´n ngá»¯ open-source Ä‘Æ°á»£c táº¡o ra bá»Ÿi Ä‘á»™i ngÅ© nghiÃªn cá»©u cá»§a Microsoft.

Báº£n phÃ¡t hÃ nh ban Ä‘áº§u bao gá»“m ba mÃ´ hÃ¬nh embedding â€” small, base, vÃ  large; phiÃªn báº£n instruct Ä‘Æ°á»£c phÃ¡t hÃ nh vÃ o Ä‘áº§u nÄƒm 2024. Xem kÃ­ch thÆ°á»›c embedding dÆ°á»›i Ä‘Ã¢y:

#### GitHub Source

#### VÃ­ dá»¥ sá»­ dá»¥ng:

```python
from sentence_transformers import SentenceTransformer

def e5_embed(query: str, model: str):
    if model not in ['large', 'base', 'small', 'large-instruct']:
        raise ValueError(f'Invalid model name {model}')

    embedder = SentenceTransformer(f'intfloat/multilingual-e5-{model}')
    if model == 'large-instruct':
        task = 'Given a short informative text, retrieve relevant topics'
        query = f'Instruct: {task}\nQuery: {query}'

    embeddings = embedder.encode(sentences=[query], convert_to_tensor=False, normalize_embeddings=True)
    return embeddings

# Example usage (768 dimensions)
embeddings = e5_embed('This is a text I want to embed', model='base')
```

Xem bÃ¡o cÃ¡o ká»¹ thuáº­t: [https://arxiv.org/pdf/2402.05672.pdf](https://arxiv.org/pdf/2402.05672.pdf)

MÃ£ GitHub: [https://github.com/microsoft/unilm/tree/master/e5](https://github.com/microsoft/unilm/tree/master/e5)

### BGE-M3

BGE-M3 lÃ  má»™t mÃ´ hÃ¬nh embedding Ä‘a ngÃ´n ngá»¯ open-source Ä‘Æ°á»£c táº¡o ra bá»Ÿi Viá»‡n TrÃ­ tuá»‡ NhÃ¢n táº¡o Báº¯c Kinh.

MÃ´ hÃ¬nh embedding má»›i nháº¥t cá»§a há», `BGE-M3`, Ä‘Æ°á»£c phÃ¡t hÃ nh vÃ o ngÃ y 30 thÃ¡ng 1, 2024, lÃ  má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ vÃ  tráº£ vá» embedding vá»›i 1024 dimensions.

M3 Ä‘áº¡i diá»‡n cho Multi-linguality (100+ ngÃ´n ngá»¯), Multi-granularities (Ä‘á»™ dÃ i Ä‘áº§u vÃ o lÃªn tá»›i 8192), Multi-Functionality (há»£p nháº¥t cá»§a dense, lexical, multi-vec (colbert) retrieval) [source].

#### VÃ­ dá»¥ sá»­ dá»¥ng:

```python
from FlagEmbedding import BGEM3FlagModel

def bge_m3_embed(query: str):
    # CÃ³ thá»ƒ thÃªm "use_fp16=True" Ä‘á»ƒ tÄƒng tá»‘c dá»± Ä‘oÃ¡n
    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)
    embeddings = model.encode([query])['dense_vecs'][0]
    return embeddings

# Example usage (1024 dimensions)
embeddings = bge_m3_embed("This is a text I want to embed")
```

Xem bÃ¡o cÃ¡o ká»¹ thuáº­t: [https://arxiv.org/pdf/2402.03216.pdf](https://arxiv.org/pdf/2402.03216.pdf)

### How to Evaluate Embedding Quality ğŸŒŸ

Má»¥c Ä‘Ã­ch cá»§a sentence embeddings lÃ  Ä‘á»ƒ bao quÃ¡t Ã½ nghÄ©a ngá»¯ nghÄ©a cá»§a toÃ n bá»™ cÃ¢u vÃ o cÃ¡c biá»ƒu diá»…n vector dense, hiá»‡u quáº£ chuyá»ƒn Ä‘á»•i tá»« ngá»¯ vÃ  ngá»¯ cáº£nh cá»§a chÃºng thÃ nh má»™t dáº¡ng sá»‘ mÃ  mÃ¡y tÃ­nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.

Vá» báº£n cháº¥t, embeddings chá»‰ lÃ  cÃ¡c biá»ƒu diá»…n sá»‘ cá»§a cÃ¢u, nÆ¡i cÃ¡c cÃ¢u cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ä‘áº·t gáº§n nhau trong khÃ´ng gian vector.

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cá»§a má»™t mÃ´ hÃ¬nh embedding báº±ng cÃ¡ch embedding cÃ¡c cÃ¢u tÆ°Æ¡ng tá»± vá» Ã½ nghÄ©a ngá»¯ nghÄ©a vÃ  Ä‘o lÆ°á»ng xem cÃ¡c embeddings nÃ y gáº§n nhau Ä‘áº¿n má»©c nÃ o.

Äá»ƒ minh há»a khÃ¡i niá»‡m, hÃ£y xem xÃ©t hai cÃ¢u sau:

â€œThe weather is chilly and windy today.â€
â€œItâ€™s a cold and breezy day.â€
Cáº£ hai cÃ¢u Ä‘á»u truyá»n táº£i Ã½ nghÄ©a ráº¥t giá»‘ng nhau báº±ng cÃ¡ch mÃ´ táº£ Ä‘iá»u kiá»‡n thá»i tiáº¿t lÃ  láº¡nh vá»›i giÃ³. LÃ½ tÆ°á»Ÿng lÃ  embedding hai cÃ¢u nÃ y nÃªn dáº«n Ä‘áº¿n cÃ¡c vector ráº¥t giá»‘ng nhau.

Äá»ƒ hiá»ƒu hiá»‡u quáº£ hoáº·c cháº¥t lÆ°á»£ng cá»§a cÃ¡c embeddings nÃ y, Ä‘iá»u quan trá»ng lÃ  pháº£i xem xÃ©t cÃ¡ch chÃºng ta cÃ³ thá»ƒ Ä‘o lÆ°á»ng khoáº£ng cÃ¡ch hoáº·c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a chÃºng, vÃ  Ä‘Ã¢y lÃ  nÆ¡i cÃ¡c hÃ m khoáº£ng cÃ¡ch (distance functions) trá»Ÿ nÃªn quan trá»ng.

CÃ³ ráº¥t nhiá»u hÃ m khoáº£ng cÃ¡ch mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng, má»—i hÃ m phá»¥c vá»¥ cho má»™t má»¥c Ä‘Ã­ch riÃªng.

DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch má»™t sá»‘ hÃ m khoáº£ng cÃ¡ch Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t:

#### Euclidean Distance
TÃ­nh khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng giá»¯a hai Ä‘iá»ƒm trong khÃ´ng gian vector. NÃ³ nháº¡y cáº£m vá»›i Ä‘á»™ lá»›n cá»§a cÃ¡c vector, vá»›i khoáº£ng cÃ¡ch nhá» hÆ¡n chá»‰ ra sá»± tÆ°Æ¡ng Ä‘á»“ng lá»›n hÆ¡n.
![alt text](images-2/image-3.png)

#### Manhattan Distance
TÃ­nh khoáº£ng cÃ¡ch giá»¯a hai tá»a Ä‘á»™ báº±ng cÃ¡ch di chuyá»ƒn theo cÃ¡c lÆ°á»›i tháº³ng (khÃ´ng pháº£i Ä‘Æ°á»ng ngáº¯n nháº¥t). NÃ³ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch tá»•ng cÃ¡c khÃ¡c biá»‡t tuyá»‡t Ä‘á»‘i cá»§a cÃ¡c tá»a Ä‘á»™ trong má»—i chiá»u.
![alt text](images-2/image-2.png)

#### Cosine Similarity
TÃ­nh cosine cá»§a gÃ³c giá»¯a hai vector, táº­p trung vÃ o hÆ°á»›ng thay vÃ¬ Ä‘á»™ lá»›n. NÃ³ lÃ½ tÆ°á»Ÿng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a trong vÄƒn báº£n, vÃ¬ nÃ³ Ä‘Ã¡nh giÃ¡ hÆ°á»›ng cá»§a cÃ¡c embeddings. GiÃ¡ trá»‹ gáº§n 1 chá»‰ ra sá»± tÆ°Æ¡ng Ä‘á»“ng cao. GiÃ¡ trá»‹ gáº§n 0 hoáº·c Ã¢m chá»‰ ra sá»± tÆ°Æ¡ng Ä‘á»“ng tháº¥p hoáº·c khÃ´ng cÃ³, vá»›i cÃ¡c giÃ¡ trá»‹ Ã¢m gá»£i Ã½ Ã½ nghÄ©a Ä‘á»‘i láº­p.

![alt text](images-2/image-4.png)
---

Viá»‡c sá»­ dá»¥ng cÃ¡c hÃ m khoáº£ng cÃ¡ch nÃ y giÃºp chÃºng ta xÃ¡c Ä‘á»‹nh cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´ hÃ¬nh embedding báº±ng cÃ¡ch Ä‘o lÆ°á»ng má»©c Ä‘á»™ gáº§n gÅ©i cá»§a cÃ¡c cÃ¢u cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± trong khÃ´ng gian vector.

### Máº·c dÃ¹ Euclidean distance ban Ä‘áº§u cÃ³ váº» nhÆ° lÃ  lá»±a chá»n há»£p lÃ½ nháº¥t, nhÆ°ng nÃ³ gáº·p pháº£i nhá»¯ng thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ trong cÃ¡c khÃ´ng gian nhiá»u chiá»u. KhÃ³ khÄƒn nÃ y phÃ¡t sinh tá»« hiá»‡n tÆ°á»£ng Ä‘Æ°á»£c gá»i lÃ  â€œlá»i nguyá»n chiá»u caoâ€ (curse of dimensionality).

Äá»ƒ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a cÃ¡c dá»¯ liá»‡u vÄƒn báº£n, cosine similarity thÆ°á»ng lÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Æ°u tiÃªn trong ba phÆ°Æ¡ng phÃ¡p, chá»§ yáº¿u do tÃ­nh Ä‘á»™c láº­p cá»§a nÃ³ vá»›i Ä‘á»™ lá»›n cá»§a vector.

Lá»£i tháº¿ nÃ y xuáº¥t phÃ¡t tá»« viá»‡c cosine similarity tÃ­nh cosine cá»§a gÃ³c giá»¯a hai vector, táº­p trung vÃ o hÆ°á»›ng mÃ  cÃ¡c vector chá»‰ thay vÃ¬ Ä‘á»™ dÃ i cá»§a chÃºng.

Cosine similarity hiá»‡u quáº£ trong viá»‡c chuáº©n hÃ³a Ä‘á»™ lá»›n cá»§a vector, lÃ m cho nÃ³ Ä‘áº·c biá»‡t phÃ¹ há»£p Ä‘á»ƒ so sÃ¡nh cÃ¡c vÄƒn báº£n cÃ³ Ä‘á»™ dÃ i vÃ  máº­t Ä‘á»™ ná»™i dung khÃ¡c nhau.

VÃ¬ váº­y, chÃºng tÃ´i sáº½ sá»­ dá»¥ng cosine similarity Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c embeddings.

ChÃºng tÃ´i sáº½ truyá»n má»™t loáº¡t cÃ¡c cÃ¢u báº±ng nhiá»u ngÃ´n ngá»¯ vÃ o mÃ´-Ä‘un Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh embedding nÃ o mang láº¡i hiá»‡u suáº¥t tá»‘t nháº¥t â€” nghÄ©a lÃ  mÃ´ hÃ¬nh chÃ­nh xÃ¡c nháº¥t trong viá»‡c náº¯m báº¯t cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a trong cÃ¡c ngá»¯ cáº£nh ngÃ´n ngá»¯ khÃ¡c nhau.

### Evaluation Dataset ğŸ”¢

TÃ´i Ä‘Ã£ táº¡o má»™t bá»™ dá»¯ liá»‡u chá»§ Ä‘á» mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cho viá»‡c Ä‘Ã¡nh giÃ¡.

Bá»™ dá»¯ liá»‡u bao gá»“m 200 cÃ¢u Ä‘Æ°á»£c phÃ¢n loáº¡i dÆ°á»›i 50 chá»§ Ä‘á» khÃ¡c nhau (má»™t sá»‘ trong Ä‘Ã³ cÃ³ liÃªn quan cháº·t cháº½).

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥ tá»« bá»™ dá»¯ liá»‡u:

- **Sailing and boating**: Many people find solace in the peacefulness of being out on the water.
- **The global economy**: Trade tensions between major economies continue to impact global markets.
- **The US stock market**: Wall Streetâ€™s indices reflect the pulse of American corporate and economic health.
- **Book reviews**: Discover the latest book reviews on bestsellers and hidden gems.
- **Religion and spirituality**: Many people find comfort and guidance in their religious beliefs.
- **AI for coding and software development**: The use of AI in software development is revolutionizing the industry.

TÃ´i Ä‘Ã£ sá»­ dá»¥ng GPT-4 Ä‘á»ƒ dá»‹ch bá»™ dá»¯ liá»‡u nÃ y sang nhiá»u ngÃ´n ngá»¯: Tiáº¿ng Anh ğŸ‡ºğŸ‡¸, Tiáº¿ng TÃ¢y Ban Nha ğŸ‡ªğŸ‡¸, Tiáº¿ng PhÃ¡p ğŸ‡«ğŸ‡·, Tiáº¿ng Äá»©c ğŸ‡©ğŸ‡ª, Tiáº¿ng Nga ğŸ‡·ğŸ‡º, Tiáº¿ng HÃ  Lan ğŸ‡³ğŸ‡±, Tiáº¿ng Bá»“ ÄÃ o Nha ğŸ‡µğŸ‡¹, Tiáº¿ng Na Uy ğŸ‡³ğŸ‡´, Tiáº¿ng Thá»¥y Äiá»ƒn ğŸ‡¸ğŸ‡ª, Tiáº¿ng Pháº§n Lan ğŸ‡«ğŸ‡®.

Go and check out the dataset for yourself: 'topic dataset - (https://github.com/LarsChrWiik/lars_datasets/tree/main/topics_dataset_50). 

### Rank-Based Evaluation â€” Top N ğŸ“ˆ

TrÆ°á»›c khi tÃ´i trÃ¬nh bÃ y káº¿t quáº£ cuá»‘i cÃ¹ng tá»« viá»‡c Ä‘Ã¡nh giÃ¡ cá»§a mÃ¬nh, cÃ¡c metric Ä‘Æ°á»£c hiá»ƒn thá»‹ cáº§n Ä‘Æ°á»£c giáº£i thÃ­ch rÃµ rÃ ng.

Cumulative Match Characteristic (CMC) curve lÃ  má»™t cÃ¡ch hiá»ƒn thá»‹ trá»±c quan Ä‘á»ƒ thá»ƒ hiá»‡n má»©c Ä‘á»™ hiá»‡u quáº£ cá»§a má»™t mÃ´ hÃ¬nh embedding trÃªn má»™t phá»• cÃ¡c ká»‹ch báº£n.

ÄÆ°á»ng cong nÃ y ráº¥t quan trá»ng trong viá»‡c trÃ¬nh bÃ y kháº£ nÄƒng dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh (trong trÆ°á»ng há»£p nÃ y lÃ  embedding phÃ¹ há»£p vá»›i má»™t truy váº¥n nháº¥t Ä‘á»‹nh) sáº½ náº±m trong cÃ¡c káº¿t quáº£ xáº¿p háº¡ng hÃ ng Ä‘áº§u (top N-ranked results).

HÃ£y xem xÃ©t má»™t vÃ­ dá»¥ khi báº¡n cÃ³ 200 cÃ¢u vÃ  Ä‘Æ°á»£c giao nhiá»‡m vá»¥ xáº¿p háº¡ng 50 chá»§ Ä‘á» dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan cá»§a chÃºng Ä‘áº¿n má»—i cÃ¢u.

ChÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c báº±ng cÃ¡ch quan sÃ¡t thá»© háº¡ng Ä‘Æ°á»£c gÃ¡n cho chá»§ Ä‘á» Ä‘Ãºng. Má»™t dá»± Ä‘oÃ¡n Ä‘áº·t chá»§ Ä‘á» Ä‘Ãºng á»Ÿ thá»© háº¡ng 1 cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c xáº¿p háº¡ng xuáº¥t sáº¯c, trong khi má»™t dá»± Ä‘oÃ¡n Ä‘áº·t chá»§ Ä‘á» Ä‘Ãºng á»Ÿ thá»© háº¡ng 50 cho tháº¥y hiá»‡u suáº¥t tá»“i tá»‡ nháº¥t cÃ³ thá»ƒ.

ChÃºng ta cÃ³ thá»ƒ láº·p qua N vá»›i 1 â‰¤ N â‰¤ 50 Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t cá»§a má»™t chá»§ Ä‘á» Ä‘Ãºng Ä‘Æ°á»£c dá»± Ä‘oÃ¡n náº±m trong top N chá»§ Ä‘á» phÃ¹ há»£p nháº¥t.

Äá»ƒ Ä‘Æ°a Ä‘iá»u nÃ y vÃ o thá»±c táº¿, hÃ£y xem xÃ©t vÃ­ dá»¥ triá»ƒn khai sau báº±ng Python:

```python
for N in range(1, 51):  # Báº¯t Ä‘áº§u tá»« 1, lÃªn Ä‘áº¿n vÃ  bao gá»“m 50
    correct_topics = []
    for query, target_topic in zip(queries, topics):
        sim = cosine_similarity(query, topics)
        sim_sorted_idx = sim.argsort()
        topics_ordered = [topics[i] for i in sim_sorted_idx]
        top_N = topics_ordered[-N:]
        correct_topics.append(1 if target_topic in top_N else 0)
    CMC_score = round(sum(correct_topics) / len(correct_topics), 3)
    print(f"Top {N} CSC_score: {CMC_score}")
```

Trong Ä‘oáº¡n mÃ£ nÃ y, `queries` Ä‘áº¡i diá»‡n cho cÃ¡c cÃ¢u cá»§a chÃºng ta, vÃ  `topics` Ä‘áº¡i diá»‡n cho cÃ¡c chá»§ Ä‘á» tiá»m nÄƒng mÃ  má»—i cÃ¢u cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n. HÃ m `cosine_similarity` Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a má»—i cÃ¢u vÃ  cÃ¡c chá»§ Ä‘á» cÃ³ thá»ƒ cÃ³. Äá»‘i vá»›i má»—i N trong pháº¡m vi xÃ¡c Ä‘á»‹nh, chÃºng ta tÃ­nh toÃ¡n má»™t Ä‘iá»ƒm sá»‘ tá»« 0 Ä‘áº¿n 1, trong Ä‘Ã³ 1 chá»‰ ra Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»§ trong viá»‡c dá»± Ä‘oÃ¡n chá»§ Ä‘á» má»¥c tiÃªu trong top N káº¿t quáº£ phÃ¹ há»£p nháº¥t.

Äá»ƒ minh há»a Cumulative Match Characteristic trÃ´ng nhÆ° tháº¿ nÃ o, hÃ£y xem vÃ­ dá»¥ cá»§a `multilingual-e5-small` cho tiáº¿ng Thá»¥y Äiá»ƒn:

---

VÃ­ dá»¥ nÃ y cho tháº¥y cÃ¡ch chÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ vÃ  so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh embedding thÃ´ng qua viá»‡c sá»­ dá»¥ng Ä‘Æ°á»ng cong CMC vÃ  Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c dá»± Ä‘oÃ¡n theo cÃ¡c thá»© háº¡ng khÃ¡c nhau.

![The CMC curve for multilingual-e5-small using the Swedish topic dataset](images-2/image-5.png)

### NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y tá»« biá»ƒu Ä‘á»“, khoáº£ng 95% cÃ¡c cÃ¢u cÃ³ chá»§ Ä‘á» Ä‘Ãºng Ä‘Æ°á»£c xáº¿p háº¡ng trong top 4 vá»‹ trÃ­ Ä‘áº§u tiÃªn â€” vÃ¬ Ä‘Æ°á»ng mÃ u xanh vÆ°á»£t qua 0.95 khi N=4.

NhÆ° báº¡n cÃ³ thá»ƒ Ä‘Ã£ hiá»ƒu, Ä‘á»™ chÃ­nh xÃ¡c chá»‰ cÃ³ thá»ƒ cáº£i thiá»‡n vá»›i cÃ¡c giÃ¡ trá»‹ N lá»›n hÆ¡n (vÃ¬ nÃ³ tÄƒng cÆ¡ há»™i chá»©a chá»§ Ä‘á» má»¥c tiÃªu).

ChÃºng ta cÃ³ thá»ƒ má»Ÿ rá»™ng biá»ƒu Ä‘á»“ nÃ y báº±ng cÃ¡ch thÃªm `multilingual-e5-large`, má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c cho lÃ  cÃ³ hiá»‡u suáº¥t tá»‘t hÆ¡n.

HÃ£y xem nÃ³ trÃ´ng nhÆ° tháº¿ nÃ o:

![The CMC curve using the Swedish topic dataset](images-2/image-6.png)

### NhÆ° mong Ä‘á»£i, `multilingual-e5-large` (Ä‘Æ°á»ng mÃ u Ä‘á») náº±m trÃªn `multilingual-e5-small` (Ä‘Æ°á»ng mÃ u xanh) â€” chá»‰ ra hiá»‡u suáº¥t tá»‘t hÆ¡n cho táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cá»§a N.

### Error Rate Evaluation ğŸ“ˆ

BÃªn cáº¡nh Ä‘Æ°á»ng cong CMC, chÃºng ta cÃ³ thá»ƒ trá»±c quan hÃ³a biá»ƒu Ä‘á»“ cá»™t vá» tá»· lá»‡ thÃ nh cÃ´ng báº±ng cÃ¡ch tÃ­nh Mean Average Precision (MAP).

Tuy nhiÃªn, Ä‘á»ƒ trá»±c quan hÃ³a Tá»· lá»‡ lá»—i (Error Rate), chÃºng ta cÃ³ thá»ƒ tÃ­nh nghá»‹ch Ä‘áº£o cá»§a MAP, Inverse Mean Average Precision (IMAP), Ä‘iá»u nÃ y sáº½ cung cáº¥p cho chÃºng ta má»™t metric táº­p trung vÃ o cÃ¡c lá»—i mÃ  mÃ´ hÃ¬nh gáº·p pháº£i thay vÃ¬ thÃ nh cÃ´ng cá»§a nÃ³.

IMAP tá»± nhiÃªn lÃ  má»™t sá»‘ nhá» vÃ¬ hiá»‡u suáº¥t trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cÃ³ thá»ƒ cá»§a N gáº§n 1. ChÃºng ta cÃ³ thá»ƒ tá»· lá»‡ lá»—i báº±ng má»™t há»‡ sá»‘ 1000 Ä‘á»ƒ Ä‘Æ°a nÃ³ vÃ o má»™t con sá»‘ dá»… hiá»ƒu hÆ¡n â€” giÃºp viá»‡c so sÃ¡nh dá»… dÃ ng hÆ¡n.

IMAP Ä‘Æ°á»£c tÃ­nh nhÆ° sau:

```python
MAP = avg(cmc_score)
scale_rate = 1000  # Äá»ƒ so sÃ¡nh sá»‘ liá»‡u dá»… dÃ ng hÆ¡n
IMAP = (1 - MAP) * scale_rate  # CÃ²n Ä‘Æ°á»£c gá»i lÃ  "tá»· lá»‡ lá»—i"
```

HÃ£y nhá»› ráº±ng khi hiá»ƒn thá»‹ tá»· lá»‡ lá»—i, cÃ¡c giÃ¡ trá»‹ gáº§n 0 hÆ¡n chá»‰ ra hiá»‡u suáº¥t tá»‘t hÆ¡n.

Äá»ƒ minh há»a Inverse Mean Average Precision trÃ´ng nhÆ° tháº¿ nÃ o, hÃ£y xem láº¡i vÃ­ dá»¥ cá»§a chÃºng ta vá»›i `multilingual-e5-small` vÃ  `multilingual-e5-large` cho tiáº¿ng Thá»¥y Äiá»ƒn:

```python
import matplotlib.pyplot as plt

# Giáº£ sá»­ cmc_scores lÃ  danh sÃ¡ch cÃ¡c Ä‘iá»ƒm sá»‘ CMC cho má»—i giÃ¡ trá»‹ cá»§a N
cmc_scores_small = [...]  # CMC scores for multilingual-e5-small
cmc_scores_large = [...]  # CMC scores for multilingual-e5-large

map_small = sum(cmc_scores_small) / len(cmc_scores_small)
map_large = sum(cmc_scores_large) / len(cmc_scores_large)

imap_small = (1 - map_small) * 1000
imap_large = (1 - map_large) * 1000

# Váº½ biá»ƒu Ä‘á»“ cá»™t cho IMAP
models = ['multilingual-e5-small', 'multilingual-e5-large']
imap_values = [imap_small, imap_large]

plt.bar(models, imap_values, color=['blue', 'red'])
plt.xlabel('Embedding Model')
plt.ylabel('Inverse Mean Average Precision (IMAP)')
plt.title('Error Rate Comparison for Swedish')
plt.show()
```

Biá»ƒu Ä‘á»“ nÃ y sáº½ cho tháº¥y tá»· lá»‡ lá»—i cá»§a hai mÃ´ hÃ¬nh `multilingual-e5-small` vÃ  `multilingual-e5-large`, giÃºp dá»… dÃ ng so sÃ¡nh hiá»‡u suáº¥t cá»§a chÃºng.

![alt text](images-2/image-7.png)

### NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, biá»ƒu Ä‘á»“ cho tháº¥y lá»—i tÃ­ch lÅ©y cá»§a má»—i mÃ´ hÃ¬nh embedding. NhÆ° mong Ä‘á»£i, `multilingual-e5-large` cÃ³ tá»· lá»‡ lá»—i tháº¥p hÆ¡n so vá»›i phiÃªn báº£n nhá» hÆ¡n cá»§a nÃ³ lÃ  `multilingual-e5-small` â€” chá»‰ ra ráº±ng `multilingual-e5-large` lÃ  mÃ´ hÃ¬nh tá»•ng thá»ƒ tá»‘t hÆ¡n cho tiáº¿ng Thá»¥y Äiá»ƒn.

### BÃ¢y giá», khi chÃºng ta Ä‘Ã£ hiá»ƒu hai metric CMC vÃ  IMAP, hÃ£y chuyá»ƒn sang káº¿t quáº£.

### Results ğŸ“Š

Cuá»™c Ä‘iá»u tra cá»§a chÃºng tÃ´i Ä‘Ã£ bao gá»“m nhiá»u ngÃ´n ngá»¯ (Tiáº¿ng Anh ğŸ‡ºğŸ‡¸, Tiáº¿ng TÃ¢y Ban Nha ğŸ‡ªğŸ‡¸, Tiáº¿ng PhÃ¡p ğŸ‡«ğŸ‡·, Tiáº¿ng Äá»©c ğŸ‡©ğŸ‡ª, Tiáº¿ng Nga ğŸ‡·ğŸ‡º, Tiáº¿ng HÃ  Lan ğŸ‡³ğŸ‡±, Tiáº¿ng Bá»“ ÄÃ o Nha ğŸ‡µğŸ‡¹, Tiáº¿ng Na Uy ğŸ‡³ğŸ‡´, Tiáº¿ng Thá»¥y Äiá»ƒn ğŸ‡¸ğŸ‡ª, vÃ  Tiáº¿ng Pháº§n Lan ğŸ‡«ğŸ‡®).

NghiÃªn cá»©u nÃ y sá»­ dá»¥ng cÃ¡c Ä‘Æ°á»ng cong Cumulative Match Characteristic vÃ  tá»· lá»‡ Inverse Mean Average Precision nhÆ° Ä‘Ã£ mÃ´ táº£ á»Ÿ cÃ¡c pháº§n trÆ°á»›c, Ä‘Ã¢y lÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ Ä‘o lÆ°á»ng má»©c Ä‘á»™ hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i.

HÃ£y xem cÃ¡c metric nÃ y khi chÃºng ta tÃ­nh trung bÃ¬nh chÃºng trÃªn táº¥t cáº£ cÃ¡c ngÃ´n ngá»¯!

### Cumulative Match Characteristic (CMC) curve

![Cumulative Match Characteristic curve â€” an average of all languages â€” higher is better](images-2/image-8.png)

Má»™t Ä‘Æ°á»ng cong CMC cao hÆ¡n chá»‰ ra hiá»‡u suáº¥t Æ°u viá»‡t. NhÆ° chÃºng ta cÃ³ thá»ƒ suy luáº­n tá»« biá»ƒu Ä‘á»“, cÃ³ sá»± phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c mÃ´ hÃ¬nh embedding hoáº¡t Ä‘á»™ng tá»“i nháº¥t vÃ  tá»‘t nháº¥t.

BÃ¢y giá», hÃ£y káº¿t há»£p cÃ¡c Ä‘iá»ƒm sá»‘ tá»« biá»ƒu Ä‘á»“ cho táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cá»§a N vÃ  tÃ­nh toÃ¡n IMAP (tá»· lá»‡ lá»—i).

Inverse Mean Average Precision (Tá»· lá»‡ lá»—i)

![Barchart of Scaled Inverse Mean Average Precision â€” an average of all languages â€” lower is better](images-2/image-9.png)

Má»™t Ä‘Æ°á»ng cong CMC cao hÆ¡n chá»‰ ra hiá»‡u suáº¥t Æ°u viá»‡t. NhÆ° chÃºng ta cÃ³ thá»ƒ suy luáº­n tá»« biá»ƒu Ä‘á»“, cÃ³ sá»± phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c mÃ´ hÃ¬nh embedding hoáº¡t Ä‘á»™ng tá»“i nháº¥t vÃ  tá»‘t nháº¥t.

BÃ¢y giá», hÃ£y káº¿t há»£p cÃ¡c Ä‘iá»ƒm sá»‘ tá»« biá»ƒu Ä‘á»“ cho táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cá»§a N vÃ  tÃ­nh toÃ¡n IMAP (tá»· lá»‡ lá»—i).

### Inverse Mean Average Precision (Tá»· lá»‡ lá»—i)

![Line plot of Inverse Mean Average Precision â€” average of all languages â€” lower is better](images-2/image-10.png)

### Interpretation

Cuá»™c Ä‘iá»u tra cá»§a chÃºng tÃ´i nháº±m xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh embedding hÃ ng Ä‘áº§u trong sá»‘ cÃ¡c ngÃ´n ngá»¯ Ä‘Æ°á»£c chá»n.

Ban Ä‘áº§u, chÃºng tÃ´i nháº­n tháº¥y ráº±ng OpenAI, Google, E5-Instruct vÃ  Cohere ná»•i báº­t so vá»›i cÃ¡c Ä‘á»‘i thá»§ khÃ¡c, vá»›i OpenAI nhá»‰nh hÆ¡n má»™t chÃºt do tá»· lá»‡ lá»—i trung bÃ¬nh tháº¥p hÆ¡n trÃªn táº¥t cáº£ cÃ¡c ngÃ´n ngá»¯.

Khi xem xÃ©t ká»¹ lÆ°á»¡ng hÆ¡n vá» hiá»‡u suáº¥t cá»§a tá»«ng ngÃ´n ngá»¯, cÃ¢u chuyá»‡n trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n.

Cohere ná»•i báº­t nhÆ° má»™t mÃ´ hÃ¬nh hÃ ng Ä‘áº§u trong má»™t sá»‘ ngÃ´n ngá»¯, vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh khÃ¡c trong má»™t ná»­a sá»‘ ngÃ´n ngá»¯ Ä‘Æ°á»£c thá»­ nghiá»‡m. Tuy nhiÃªn, Ä‘Ã¡ng chÃº Ã½ ráº±ng nÃ³ hiá»‡u suáº¥t kÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ trong tiáº¿ng Bá»“ ÄÃ o Nha, Ä‘iá»u nÃ y gá»£i ra má»™t cÃ¡i nhÃ¬n rá»™ng hÆ¡n vá» sá»± nháº¥t quÃ¡n cá»§a hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t cÃ¡i nhÃ¬n tá»•ng quan vá» mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cho má»—i ngÃ´n ngá»¯:

![alt text](images-2/image-11.png)

Máº·c dÃ¹ Cohere hoáº¡t Ä‘á»™ng tá»‘t trong má»™t sá»‘ ngÃ´n ngá»¯ nháº¥t Ä‘á»‹nh, má»™t mÃ´ hÃ¬nh embedding Ä‘a ngÃ´n ngá»¯ tá»‘i Æ°u nÃªn cho tháº¥y káº¿t quáº£ nháº¥t quÃ¡n trÃªn má»™t loáº¡t rá»™ng lá»›n cÃ¡c ngÃ´n ngá»¯ Ä‘a dáº¡ng.

OpenAI lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh vá» sá»± nháº¥t quÃ¡n nÃ y khi hiá»‡u suáº¥t cá»§a nÃ³ váº«n á»•n Ä‘á»‹nh qua cÃ¡c phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i. Äiá»u nÃ y gá»£i ra ráº±ng OpenAI xuáº¥t sáº¯c trong viá»‡c táº¡o ra cÃ¡c embedding máº¡nh máº½, cÃ³ tÃ­nh á»©ng dá»¥ng phá»• quÃ¡t.

Tuy nhiÃªn, cáº§n lÆ°u Ã½ ráº±ng táº­p dá»¯ liá»‡u chá»§ Ä‘á» Ä‘Æ°á»£c táº¡o ra bá»Ÿi GPT4 (cá»§a OpenAI), Ä‘iá»u nÃ y cÃ³ thá»ƒ gÃ¢y ra Ä‘á»™ chá»‡ch hiá»‡u suáº¥t khi sá»­ dá»¥ng mÃ´ hÃ¬nh embedding cá»§a OpenAI. Vá»›i nhá»¯ng Ä‘iá»u Ä‘Ã³, Cohere váº«n vÆ°á»£t trá»™i so vá»›i OpenAI trong háº§u háº¿t cÃ¡c ngÃ´n ngá»¯ nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong báº£ng á»Ÿ trÃªn.

Quan sÃ¡t nÃ y gá»£i ra ráº±ng viá»‡c bao gá»“m nhiá»u ngÃ´n ngá»¯ hÆ¡n trong cÃ¡c phÃ¢n tÃ­ch trong tÆ°Æ¡ng lai cÃ³ thá»ƒ lÃ m ná»•i báº­t hÆ¡n cÃ¡c Ä‘iá»ƒm máº¡nh cá»§a cÃ¡c mÃ´ hÃ¬nh, cá»§ng cá»‘ sá»± quan trá»ng cá»§a tÃ­nh linh hoáº¡t vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cá»§a chÃºng trÃªn má»™t pháº¡m vi ngÃ´n ngá»¯ rá»™ng lá»›n.

### Káº¿t luáº­n ğŸ’¡

PhÃ¢n tÃ­ch nÃ y lÃ m sÃ¡ng tá» vá» cáº£nh quan phá»©c táº¡p cá»§a cÃ¡c cÃ´ng nghá»‡ embedding Ä‘a ngÃ´n ngá»¯.

Trong khi OpenAI, Google vÃ  Cohere lÃ  cÃ¡c lá»±c lÆ°á»£ng dáº«n Ä‘áº§u trong lÄ©nh vá»±c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘á»™c quyá»n, phÃ¢n tÃ­ch nÃ y thá»ƒ hiá»‡n hiá»‡u suáº¥t tá»‘i Æ°u cá»§a há» trong viá»‡c táº¡o ra embedding Ä‘a ngÃ´n ngá»¯.

Äáº·c biá»‡t Ä‘Ã¡ng chÃº Ã½ lÃ  sá»± á»•n Ä‘á»‹nh cá»§a OpenAI qua cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau, hiá»‡u suáº¥t máº¡nh máº½ cá»§a Cohere trong má»™t nhÃ³m ngÃ´n ngá»¯ Ä‘Æ°á»£c chá»n vÃ  hiá»‡u suáº¥t tá»•ng thá»ƒ cá»§a Google vá»›i kÃ­ch thÆ°á»›c embedding nhá».

HÆ¡n ná»¯a, tÃ¡c Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ nhÆ° E5-Instruct vÃ  BGE-M3 khÃ´ng thá»ƒ bá»‹ coi thÆ°á»ng. Nhá»¯ng mÃ´ hÃ¬nh nÃ y khÃ´ng chá»‰ lÃ m phong phÃº cáº£nh quan cáº¡nh tranh mÃ  cÃ²n thá»ƒ hiá»‡n tiá»m nÄƒng cá»§a sá»± Ä‘á»•i má»›i dá»±a trÃªn cá»™ng Ä‘á»“ng trong lÄ©nh vá»±c AI.

TÃ´i muá»‘n bÃ y tá» lÃ²ng biáº¿t Æ¡n Ä‘áº¿n cÃ¡c nhÃ³m nghiÃªn cá»©u tá»« Microsoft cho cÃ¡c mÃ´ hÃ¬nh E5 cÅ©ng nhÆ° Viá»‡n TrÃ­ tuá»‡ NhÃ¢n táº¡o Báº¯c Kinh cho BGE-M3. ÄÃ³ng gÃ³p Ä‘Ã¡ng ká»ƒ cá»§a há» lÃ m phong phÃº thÃªm cáº£nh quan cáº¡nh tranh.